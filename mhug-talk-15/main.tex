\documentclass[10pt]{beamer}
\usepackage{textcomp}
\usepackage{minted}

\usetheme{m}
\title{Funn: Functional Neural Networks in Haskell}
\subtitle{(everything's a category)}
\date{\today}
\author{Neil Shepperd}

\newenvironment{xframe}[1][]{\begin{frame}[fragile,environment=xframe,#1]}{\end{frame}}

\begin{document}
\maketitle

%% \section{The Problem}
\begin{xframe}{The Problem}
  A neural network should...
  \pause
  \begin{enumerate}
  \item Take some input $\mathbf{x}$ \pause
  \item Take some parameters $\mathbf{W}$ \pause
  \item Produce an output ($\mathbf{y}, \mathcal{L}$) \pause
  \item Produce a gradient $\frac{d\mathcal{L}}{d(\mathbf{W},\mathbf{x})}$
  \end{enumerate}
\end{xframe}

\begin{xframe}{The Solution?}
  Didn't I just describe the space of differentiable functions? \\
  There's an ekmett library for that\texttrademark \pause --- \textbf{ad}.

  \begin{minted}{haskell}
    > diff sin 0
    1.0
  \end{minted}

  Great, we can all go home.

  Except...
  \pause

  The performance on thousands of variables is not so great.
  And we want to be able to export to GPU.
\end{xframe}

\begin{xframe}{Differentiable Functions}
  Let's backtrack a bit.

  Differentiable functions \emph{do} form a category.

  \[ \frac{d}{dx} \text{id}(x) = 1 \]
  \[ \frac{d}{dx} (g \circ f)(x) = f'(x) g'(f(x)) \]

  (A subcategory of ``functions between vector spaces''.)
\end{xframe}

\begin{xframe}{A Category}
  So let's build a category interface.

  \begin{minted}{haskell}
data Network a b = ...

id :: Network a a
(>>>) :: Network a b -> Network b c -> Network a c
  \end{minted}
\end{xframe}

\begin{xframe}{Networks}
  I spent quite a while experimenting with different designs.

  Still not finished yet. But right now:

  \begin{minted}{haskell}
    newtype Parameters = Parameters (S.Vector Double)
    class Derivable s where
        type family D s :: *
    data Network m a b = Network {
      evaluate :: Parameters -> a -> m (b, Double,
                        D b -> m (D a, [D Parameters])),
      params :: Int,
      initialise :: RVar Parameters
    }
  \end{minted}

  Monad, allowing effects -- eg. randomness for dropout units.

  Take parameters, produce output together with contribution to loss function and a callback to calculate gradient on the way back.
\end{xframe}

%% \begin{xframe}{I'm not done}
%%   Most of this stuff not necessary:

%%   \begin{minted}{haskell}
%%     data Network m a b = Network {
%%       evaluate :: a -> m (b, D b -> m (D a))
%%     }
%%   \end{minted}

%%   could suffice.
%% \end{xframe}

\begin{xframe}{Category Interface}
  \begin{minted}{haskell}
data Network m a b = ...

id :: Network m a a
(>>>) :: Network m a b -> Network m b c -> Network m a c
  \end{minted}
\end{xframe}

\begin{xframe}{Category Interface -- id}
  \begin{minted}{haskell}
    id :: (Monad m) => Network m a a
    id = Network ev 0 (return mempty)
     where
      ev _ a = return (a, 0, backward)
      backward b = return (b, [])
  \end{minted}
\end{xframe}

\begin{xframe}{Category Interface -- ($>>>$)}
  \begin{minted}{haskell}
(>>>) :: (Monad m) => Network m a b -> Network m b c -> Network m a c
(>>>) one two = Network ev (params one + params two) ...
    where ev (Parameters par) !a =
      do let par1 = Parameters (V.take (params one) par)
             par2 = Parameters (V.drop (params one) par)
         (!b, !cost1, !k1) <- evaluate one par1 a
         (!c, !cost2, !k2) <- evaluate two par2 b
         let backward !dc = do (!db, dpar2) <- k2 dc
                               (!da, dpar1) <- k1 db
                               return (da, dpar1 <> dpar2)
         return (c, cost1 + cost2, backward)
  \end{minted}
\end{xframe}

\begin{xframe}{Monoidal Category}
  We're really a sort of monoidal category:

  \begin{minted}{haskell}
    left :: Network m a b -> Network m (a,c) (b,c)
    right :: Network m a b -> Network m (c,a) (c,b)
    (***) :: Network m a b -> Network m c d ->
              Network m (a,c) (b,d)
    assocL :: Network m (a,(b,c)) ((a,b),c)
    assocR :: Network m ((a,b),c) (a,(b,c))
    swap :: Network m (a,b) (b,a)
  \end{minted}

\end{xframe}

\begin{xframe}{Data -- Statically Checked Dimensions}
  Usual unit of data storage for plain neural networks is a fixed length vector.

  \begin{minted}{haskell}
    import GHC.TypeLits
    import qualified Data.Vector.Storable as S
    data Blob (n :: Nat) = Blob { getBlob :: S.Vector Double }
    instance Derivable (Blob n) where
        type D (Blob n) = Blob n
  \end{minted}

  Using type-level nats we ensure correct construction of the network.

  And - dimensions for each layer can sometimes be inferred
\end{xframe}

\begin{xframe}{Data -- Basic Operations}
  \begin{minted}{haskell}
    fcLayer :: Network m (Blob n1) (Blob n2)
    sigmoidLayer :: Network m (Blob n) (Blob n)
    quadraticCost :: Network m (Blob n, Blob n) ()
    crossEntropyCost :: Network m (Blob n, Blob n) ()
    softmaxCost :: Network m (Blob n, Int) ()
  \end{minted}

  Softmax is not entirely safe since the domain of the Int is not constrained...
\end{xframe}

\begin{xframe}{Using this}
  \begin{minted}{haskell}
    type Layer = Network Identity
    let network :: Layer (Blob 2) (Blob 1)
        network = (fcLayer :: Layer (Blob 2) (Blob 3) >>>
                  sigmoidLayer                        >>>
                  (fcLayer :: Layer (Blob 3) (Blob 1))

    let training :: Layer (Blob 2, Blob 1) ()
        training = left network >>> quadraticCost
  \end{minted}
\end{xframe}


\end{document}
